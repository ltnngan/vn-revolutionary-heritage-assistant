{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1042c78f",
   "metadata": {},
   "source": [
    "Running with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee249b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 1. Cài đặt thư viện\n",
    "# ========================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "!pip install -q transformers==4.45.0 datasets peft accelerate bitsandbytes\n",
    "!pip install -q pillow torch torchvision qwen-vl-utils\n",
    "!pip install -q rouge-score bert-score nltk\n",
    "!pip install -q git+https://github.com/salaniz/pycocoevalcap.git\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 2. Import thư viện\n",
    "# ========================================================================\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers import EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from pycocoevalcap.cider.cider import Cider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 3. Config\n",
    "# ========================================================================\n",
    "\n",
    "class Config:\n",
    "    MODEL_NAME = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "    DATA_PATH = \"/kaggle/input/dataset/vn-revolutionary-heritage-vqa.json\"\n",
    "    IMAGE_BASE_PATH = \"/kaggle/input/images\"\n",
    "    OUTPUT_DIR = \"/kaggle/working/qwen2vl-heritage-vqa\"\n",
    "    \n",
    "    MAX_LENGTH = 640 \n",
    "    IMAGE_MAX_SIZE = 336\n",
    "    \n",
    "    TEST_SIZE = 0.10\n",
    "    VAL_SIZE = 0.10\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    EVAL_BATCH_SIZE = 8 \n",
    "    GRADIENT_ACCUMULATION = 32\n",
    "    \n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_RATIO = 0.05\n",
    "    \n",
    "    SAVE_STRATEGY = \"epoch\"\n",
    "    EVAL_STRATEGY = \"epoch\"\n",
    "    LOGGING_STEPS = 25\n",
    "    \n",
    "    LORA_R = 32\n",
    "    LORA_ALPHA = 64\n",
    "    LORA_DROPOUT = 0.1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 4. Class tính metrics\n",
    "# ========================================================================\n",
    "\n",
    "class VQAMetrics:\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True\n",
    "        )\n",
    "        self.smooth = SmoothingFunction()\n",
    "    \n",
    "    def compute_bleu(self, predictions, references):\n",
    "        scores = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_tokens = str(pred).lower().split()\n",
    "            ref_tokens = str(ref).lower().split()\n",
    "            try:\n",
    "                score = sentence_bleu([ref_tokens], pred_tokens, \n",
    "                                     smoothing_function=self.smooth.method1)\n",
    "                scores.append(score)\n",
    "            except:\n",
    "                scores.append(0.0)\n",
    "        return scores\n",
    "    \n",
    "    def compute_rouge(self, predictions, references):\n",
    "        r1, r2, rL = [], [], []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            try:\n",
    "                scores = self.rouge_scorer.score(str(ref).lower(), str(pred).lower())\n",
    "                r1.append(scores['rouge1'].fmeasure)\n",
    "                r2.append(scores['rouge2'].fmeasure)\n",
    "                rL.append(scores['rougeL'].fmeasure)\n",
    "            except:\n",
    "                r1.append(0.0)\n",
    "                r2.append(0.0)\n",
    "                rL.append(0.0)\n",
    "        return r1, r2, rL\n",
    "    \n",
    "    def compute_bertscore(self, predictions, references):\n",
    "        try:\n",
    "            P, R, F1 = bert_score(\n",
    "                [str(p) for p in predictions], \n",
    "                [str(r) for r in references], \n",
    "                lang='vi', verbose=False,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "            return F1.mean().item()\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def compute_cider(self, predictions, references):\n",
    "        try:\n",
    "            gts = {i: [str(r)] for i, r in enumerate(references)}\n",
    "            res = {i: [str(p)] for i, p in enumerate(predictions)}\n",
    "            cider_scorer = Cider()\n",
    "            score, _ = cider_scorer.compute_score(gts, res)\n",
    "            return score\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def compute_exact_match(self, predictions, references):\n",
    "        matches = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_norm = str(pred).strip().lower()\n",
    "            ref_norm = str(ref).strip().lower()\n",
    "            matches.append(1.0 if pred_norm == ref_norm else 0.0)\n",
    "        return matches\n",
    "    \n",
    "    def compute_all(self, predictions, references):\n",
    "        bleu = self.compute_bleu(predictions, references)\n",
    "        r1, r2, rL = self.compute_rouge(predictions, references)\n",
    "        bertscore = self.compute_bertscore(predictions, references)\n",
    "        cider = self.compute_cider(predictions, references)\n",
    "        exact_match = self.compute_exact_match(predictions, references)\n",
    "        \n",
    "        return {\n",
    "            'bleu': np.mean(bleu),\n",
    "            'rouge1': np.mean(r1),\n",
    "            'rouge2': np.mean(r2),\n",
    "            'rougeL': np.mean(rL),\n",
    "            'bertscore_f1': bertscore,\n",
    "            'cider': cider,\n",
    "            'exact_match': np.mean(exact_match)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d894690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 5. Xử lý data\n",
    "# ========================================================================\n",
    "\n",
    "def resize_image(image_path: str, max_size: int = 336) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode in (\"RGBA\", \"P\", \"LA\"):\n",
    "            img = img.convert(\"RGB\")\n",
    "        \n",
    "        if max(img.size) > max_size:\n",
    "            img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        return img\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_and_prepare_data(json_path, image_base_path, max_samples=None):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # GROUP theo IMAGE\n",
    "    image_groups = {}  # key: image_path, value: [samples]\n",
    "    \n",
    "    for item in data:\n",
    "        name = item['name']\n",
    "        location = item.get('location_after', item.get('location_before', ''))\n",
    "        \n",
    "        for img_data in item.get('images', []):\n",
    "            img_path = os.path.join(image_base_path, img_data['image'])\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "            \n",
    "            img = resize_image(img_path, config.IMAGE_MAX_SIZE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Key = image path để group\n",
    "            img_key = img_data['image']\n",
    "            \n",
    "            if img_key not in image_groups:\n",
    "                image_groups[img_key] = []\n",
    "            \n",
    "            for qa in img_data.get('qas', []):\n",
    "                if not qa['answer'] or len(qa['answer'].strip()) < 2:\n",
    "                    continue\n",
    "                \n",
    "                image_groups[img_key].append({\n",
    "                    'image': img,\n",
    "                    'question': qa['question'],\n",
    "                    'answer': qa['answer'],\n",
    "                    'context': f\"{name} tại {location}.\",\n",
    "                    'site_name': name,\n",
    "                    'image_path': img_key\n",
    "                })\n",
    "    \n",
    "    # SPLIT theo IMAGE groups\n",
    "    image_keys = list(image_groups.keys())\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(image_keys)\n",
    "    \n",
    "    n_images = len(image_keys)\n",
    "    n_test_imgs = int(n_images * config.TEST_SIZE)\n",
    "    n_val_imgs = int(n_images * config.VAL_SIZE)\n",
    "    \n",
    "    test_img_keys = image_keys[:n_test_imgs]\n",
    "    val_img_keys = image_keys[n_test_imgs:n_test_imgs+n_val_imgs]\n",
    "    train_img_keys = image_keys[n_test_imgs+n_val_imgs:]\n",
    "    \n",
    "    # Flatten\n",
    "    train_samples = [s for key in train_img_keys for s in image_groups[key]]\n",
    "    val_samples = [s for key in val_img_keys for s in image_groups[key]]\n",
    "    test_samples = [s for key in test_img_keys for s in image_groups[key]]\n",
    "\n",
    "    np.random.shuffle(train_samples) \n",
    "    \n",
    "    print(f\"Images - Train: {len(train_img_keys)} | Val: {len(val_img_keys)} | Test: {len(test_img_keys)}\")\n",
    "    print(f\"Samples - Train: {len(train_samples)} | Val: {len(val_samples)} | Test: {len(test_samples)}\")\n",
    "\n",
    "    # Lưu file\n",
    "    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "    test_metadata = [\n",
    "        {\n",
    "            'question': s['question'],\n",
    "            'answer': s['answer'],\n",
    "            'context': s['context'],\n",
    "            'site_name': s['site_name'],\n",
    "            'image_path': s['image_path']  \n",
    "        }\n",
    "        for s in test_samples\n",
    "    ]\n",
    "    \n",
    "    with open('/kaggle/working/qwen2vl-heritage-vqa/test_set.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✓ Đã lưu {len(test_metadata)} test samples vào test_set.json\")\n",
    "    \n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "def create_conversation_format(sample):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": sample['image']},\n",
    "                {\"type\": \"text\", \"text\": sample['question']}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample['answer']}]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad40447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 6. Data Collator\n",
    "# ========================================================================\n",
    "\n",
    "class VQADataCollator:\n",
    "    def __init__(self, processor, max_length=640):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.IGNORE_INDEX = -100\n",
    "\n",
    "        self.processor.tokenizer.padding_side = 'right'\n",
    "        \n",
    "        # Lấy token IDs\n",
    "        self.im_start_id = processor.tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "        self.im_end_id = processor.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "        self.assistant_token_ids = processor.tokenizer.encode(\"assistant\", add_special_tokens=False)\n",
    "        self.newline_id = processor.tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "\n",
    "        print(\"=\"*70)\n",
    "        print(\"DEBUG COLLATOR INIT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"im_start_id      : {self.im_start_id}\")\n",
    "        print(f\"im_end_id        : {self.im_end_id}\")\n",
    "        print(f\"assistant_tokens : {self.assistant_token_ids}\")\n",
    "        print(f\"newline_id       : {self.newline_id}\")\n",
    "        print(f\"pad_token_id     : {processor.tokenizer.pad_token_id}\")\n",
    "        print(f\"eos_token_id     : {processor.tokenizer.eos_token_id}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Validate\n",
    "        if self.im_start_id == processor.tokenizer.unk_token_id:\n",
    "            raise ValueError(\"<|im_start|> không có trong vocab\")\n",
    "        if self.im_end_id == processor.tokenizer.unk_token_id:\n",
    "            raise ValueError(\"<|im_end|> không có trong vocab\")\n",
    "    \n",
    "    def find_assistant_start(self, input_ids):\n",
    "        \"\"\"Tìm vị trí bắt đầu của assistant response\"\"\"\n",
    "        input_ids = input_ids.tolist() if torch.is_tensor(input_ids) else input_ids\n",
    "        \n",
    "        for i in range(len(input_ids) - len(self.assistant_token_ids) - 1):\n",
    "            if input_ids[i] != self.im_start_id:\n",
    "                continue\n",
    "            \n",
    "            # Kiểm tra match \"assistant\"\n",
    "            match = True\n",
    "            for j, token_id in enumerate(self.assistant_token_ids):\n",
    "                if i + 1 + j >= len(input_ids) or input_ids[i + 1 + j] != token_id:\n",
    "                    match = False\n",
    "                    break\n",
    "            \n",
    "            if not match:\n",
    "                continue\n",
    "            \n",
    "            pos = i + 1 + len(self.assistant_token_ids)\n",
    "            \n",
    "            # Skip newline\n",
    "            if pos < len(input_ids) and input_ids[pos] == self.newline_id:\n",
    "                pos += 1\n",
    "            \n",
    "            # Kiểm tra answer rỗng\n",
    "            if pos >= len(input_ids) or input_ids[pos] == self.im_end_id:\n",
    "                return None\n",
    "            \n",
    "            return pos\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        texts = []\n",
    "        images_list = []\n",
    "        \n",
    "        for feat in features:\n",
    "            messages = create_conversation_format(feat)\n",
    "            \n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "            \n",
    "            image_inputs, _ = process_vision_info(messages)\n",
    "            images_list.append(image_inputs)\n",
    "        \n",
    "        # Tokenize\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            images=images_list,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # Tạo labels\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = self.IGNORE_INDEX\n",
    "        \n",
    "        skipped = 0\n",
    "\n",
    "        debug_this_batch = not hasattr(self, '_first_batch_debugged')\n",
    "        if debug_this_batch:\n",
    "            self._first_batch_debugged = True\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"DEBUG FIRST BATCH - KIỂM TRA MASK\")\n",
    "            print(\"=\"*70)\n",
    "        \n",
    "        for i in range(len(texts)):\n",
    "            input_ids = batch[\"input_ids\"][i]\n",
    "            assistant_start = self.find_assistant_start(input_ids)\n",
    "\n",
    "            if debug_this_batch and i == 0:\n",
    "                print(f\"\\n--- SAMPLE 0 ---\")\n",
    "                print(f\"Input length: {len(input_ids)}\")\n",
    "                \n",
    "                # Decode toàn bộ\n",
    "                full_text = self.processor.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "                print(f\"\\nFULL TEXT:\")\n",
    "                print(full_text[:600]) \n",
    "                print(\"...\\n\")\n",
    "                \n",
    "                # Tìm vị trí special tokens\n",
    "                im_start_positions = [j for j, tok in enumerate(input_ids) if tok == self.im_start_id]\n",
    "                im_end_positions = [j for j, tok in enumerate(input_ids) if tok == self.im_end_id]\n",
    "                \n",
    "                print(f\"Token positions:\")\n",
    "                print(f\"   <|im_start|> at: {im_start_positions}\")\n",
    "                print(f\"   <|im_end|> at  : {im_end_positions}\")\n",
    "                print(f\"   Assistant start: {assistant_start}\")\n",
    "                \n",
    "                if assistant_start is not None:\n",
    "                    # Decode phần prompt (sẽ bị mask)\n",
    "                    prompt_tokens = input_ids[:assistant_start]\n",
    "                    prompt_text = self.processor.tokenizer.decode(prompt_tokens, skip_special_tokens=False)\n",
    "                    print(f\"\\nPROMPT (will be MASKED):\")\n",
    "                    print(f\"{prompt_text[:400]}\")\n",
    "                    print(\"...\")\n",
    "                    \n",
    "                    # Decode phần answer (trainable)\n",
    "                    answer_tokens = input_ids[assistant_start:]\n",
    "                    answer_text = self.processor.tokenizer.decode(answer_tokens, skip_special_tokens=False)\n",
    "                    print(f\"\\nANSWER (TRAINABLE):\")\n",
    "                    print(f\"{answer_text}\")\n",
    "                    \n",
    "                    # Đếm tokens\n",
    "                    valid_mask = (input_ids != self.processor.tokenizer.pad_token_id)\n",
    "                    temp_labels = labels[i].clone()\n",
    "                    temp_labels[:assistant_start] = self.IGNORE_INDEX\n",
    "                    trainable = ((temp_labels != self.IGNORE_INDEX) & valid_mask).sum().item()\n",
    "                    \n",
    "                    print(f\"\\nStatistics:\")\n",
    "                    print(f\"   Total tokens    : {len(input_ids)}\")\n",
    "                    print(f\"   Prompt tokens   : {assistant_start}\")\n",
    "                    print(f\"   Answer tokens   : {len(input_ids) - assistant_start}\")\n",
    "                    print(f\"   Trainable tokens: {trainable}\")\n",
    "                else:\n",
    "                    print(\"\\nERROR: Assistant start NOT FOUND!\")\n",
    "                    print(\"   → This sample will be SKIPPED\")\n",
    "            \n",
    "            if assistant_start is None:\n",
    "                labels[i, :] = self.IGNORE_INDEX\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Mask prompt\n",
    "            labels[i, :assistant_start] = self.IGNORE_INDEX\n",
    "            \n",
    "            # Verify có token trainable\n",
    "            valid_mask = (input_ids != self.processor.tokenizer.pad_token_id)\n",
    "            trainable = ((labels[i] != self.IGNORE_INDEX) & valid_mask).sum().item()\n",
    "            \n",
    "            if trainable == 0:\n",
    "                labels[i, :] = self.IGNORE_INDEX\n",
    "                skipped += 1\n",
    "\n",
    "        if debug_this_batch:\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        if skipped > 0:\n",
    "            print(f\"⚠ Skipped {skipped}/{len(texts)} samples trong batch\")\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        #debug\n",
    "        valid_labels = (labels != self.IGNORE_INDEX).sum(dim=1)\n",
    "        # print(f\"Valid labels per sample: {valid_labels.tolist()}\")\n",
    "        \n",
    "        if (valid_labels == 0).any():\n",
    "            print(f\"⚠ WARNING: {(valid_labels == 0).sum()} samples có 0 trainable tokens!\")\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 7. Load Model\n",
    "# ========================================================================\n",
    "\n",
    "def load_model_and_processor():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Đang load model...\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        min_pixels=config.IMAGE_MAX_SIZE**2,\n",
    "        max_pixels=config.IMAGE_MAX_SIZE**2\n",
    "    )\n",
    "    \n",
    "    # Set padding side\n",
    "    processor.tokenizer.padding_side = 'right'\n",
    "\n",
    "    if processor.tokenizer.pad_token_id is None:\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97635f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 8. Setup LoRA\n",
    "# ========================================================================\n",
    "\n",
    "def setup_lora(model):\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.LORA_R,\n",
    "        lora_alpha=config.LORA_ALPHA,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                       \"qkv\", \"proj\", \"fc1\", \"fc2\"],\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'visual' in name and 'lora' not in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    print(\"✓ LoRA applied\")\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    visual_lora = sum(p.numel() for n, p in model.named_parameters() \n",
    "                     if 'visual' in n and 'lora' in n and p.requires_grad)\n",
    "    text_lora = sum(p.numel() for n, p in model.named_parameters() \n",
    "                   if 'visual' not in n and p.requires_grad)\n",
    "    print(f\"Visual LoRA: {visual_lora:,} params\")\n",
    "    print(f\"Text LoRA:   {text_lora:,} params\")\n",
    "    print(f\"Total:       {visual_lora + text_lora:,} params\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 9. Training Arguments\n",
    "# ========================================================================\n",
    "\n",
    "def get_training_args():\n",
    "    return TrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION,\n",
    "        learning_rate=5e-6,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=config.WARMUP_RATIO,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        dataloader_num_workers=2,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_prefetch_factor=2,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_drop_last=False,\n",
    "        logging_steps=config.LOGGING_STEPS,\n",
    "        eval_strategy=config.EVAL_STRATEGY,    \n",
    "        save_strategy=config.SAVE_STRATEGY,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        report_to=\"none\",\n",
    "        save_safetensors=True,\n",
    "        disable_tqdm=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 10. Evaluation\n",
    "# ========================================================================\n",
    "\n",
    "def evaluate_model(model, processor, test_samples, batch_size=8):\n",
    "    print(f\"Bắt đầu đánh giá trên {len(test_samples)} samples\")\n",
    "\n",
    "    original_padding_side = processor.tokenizer.padding_side\n",
    "\n",
    "    processor.tokenizer.padding_side = 'left'\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "        metrics_calculator = VQAMetrics()\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        sample_indices = []\n",
    "        \n",
    "        for i in range(0, len(test_samples), batch_size):\n",
    "            batch_samples = test_samples[i:i+batch_size]\n",
    "            \n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"Đã xử lý {i}/{len(test_samples)} samples\")\n",
    "            \n",
    "            batch_texts = []\n",
    "            batch_images = []\n",
    "            batch_refs = []\n",
    "            batch_idx = []\n",
    "            \n",
    "            for j, sample in enumerate(batch_samples):\n",
    "                try:\n",
    "                    img = sample['image']\n",
    "                    \n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image\", \"image\": img},\n",
    "                                {\"type\": \"text\", \"text\": sample['question']}\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                    \n",
    "                    text = processor.apply_chat_template(\n",
    "                        messages, tokenize=False, add_generation_prompt=True\n",
    "                    )\n",
    "                    image_inputs, _ = process_vision_info(messages)\n",
    "                    \n",
    "                    batch_texts.append(text)\n",
    "                    batch_images.append(image_inputs)\n",
    "                    batch_refs.append(sample['answer'])\n",
    "                    batch_idx.append(i + j)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Lỗi sample {i+j}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if len(batch_texts) > 0:\n",
    "                try:\n",
    "                    # Flatten images list\n",
    "                    flat_images = []\n",
    "                    for img_list in batch_images:\n",
    "                        flat_images.extend(img_list)\n",
    "                    \n",
    "                    inputs = processor(\n",
    "                        text=batch_texts,\n",
    "                        images=flat_images,\n",
    "                        padding=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    ).to(model.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=256,\n",
    "                            do_sample=False,\n",
    "                            repetition_penalty=1.1,\n",
    "                            pad_token_id=processor.tokenizer.pad_token_id\n",
    "                        )\n",
    "                    \n",
    "                    for j, output in enumerate(outputs):\n",
    "                        answer = processor.decode(\n",
    "                            output[inputs['input_ids'].shape[1]:],\n",
    "                            skip_special_tokens=True\n",
    "                        )\n",
    "                        predictions.append(answer)\n",
    "                        references.append(batch_refs[j])\n",
    "                        sample_indices.append(batch_idx[j])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Lỗi inference batch {i}: {e}\")\n",
    "            \n",
    "            # Clear cache mỗi 10 batches\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KẾT QUẢ TEST SET\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        test_metrics = metrics_calculator.compute_all(predictions, references)\n",
    "        \n",
    "        for key, value in test_metrics.items():\n",
    "            print(f\"{key:15s}: {value:.4f}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'metrics': test_metrics,\n",
    "            'num_samples': len(test_samples),\n",
    "            'processed_samples': len(predictions),\n",
    "            'sample_predictions': [\n",
    "                {\n",
    "                    'question': test_samples[sample_indices[i]]['question'],\n",
    "                    'predicted': predictions[i],\n",
    "                    'reference': references[i],\n",
    "                    'image_path': test_samples[sample_indices[i]]['image_path']\n",
    "                }\n",
    "                for i in range(min(20, len(predictions)))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(f\"{config.OUTPUT_DIR}/test_results.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return test_metrics\n",
    "    finally:\n",
    "        # Restore lại setting ban đầu\n",
    "        processor.tokenizer.padding_side = original_padding_side\n",
    "        print(f\"\\n✓ Đã restore padding_side về '{original_padding_side}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ab2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 11. Main Training\n",
    "# ========================================================================\n",
    "\n",
    "def train():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"BƯỚC 1: LOAD DATA\")\n",
    "    print(\"=\"*70)\n",
    "    train_samples, val_samples, test_samples = load_and_prepare_data(\n",
    "        config.DATA_PATH, config.IMAGE_BASE_PATH\n",
    "    )\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_samples)\n",
    "    val_dataset = Dataset.from_list(val_samples)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BƯỚC 2: LOAD MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    model, processor = load_model_and_processor()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BƯỚC 3: SETUP LoRA\")\n",
    "    print(\"=\"*70)\n",
    "    model = setup_lora(model)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BƯỚC 4: CHUẨN BỊ COLLATOR\")\n",
    "    print(\"=\"*70)\n",
    "    data_collator = VQADataCollator(processor, max_length=config.MAX_LENGTH)\n",
    "    \n",
    "    # Test collator\n",
    "    print(\"Kiểm tra collator...\")\n",
    "    test_batch = data_collator([train_samples[0], train_samples[1]])\n",
    "    print(\"✓ Collator OK\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BƯỚC 5: TEST FORWARD PASS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model.eval()\n",
    "    test_sample = [train_samples[0]]\n",
    "    test_batch = data_collator(test_sample)\n",
    "    test_batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v \n",
    "                  for k, v in test_batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**test_batch)\n",
    "        test_loss = outputs.loss\n",
    "        \n",
    "        if torch.isnan(test_loss) or torch.isinf(test_loss):\n",
    "            raise RuntimeError(f\"Forward pass tạo invalid loss: {test_loss.item()}\")\n",
    "        \n",
    "        print(f\"✓ Forward pass OK, loss: {test_loss.item():.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BƯỚC 6: TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    training_args = get_training_args()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=2,      \n",
    "                early_stopping_threshold=0.005\n",
    "            )\n",
    "        ] \n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BƯỚC 7: LƯU MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    trainer.save_model(config.OUTPUT_DIR)\n",
    "    processor.save_pretrained(config.OUTPUT_DIR)\n",
    "    print(f\"✓ Đã lưu model tại {config.OUTPUT_DIR}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BƯỚC 8: ĐÁNH GIÁ TEST SET\")\n",
    "    print(\"=\"*70)\n",
    "    test_metrics = evaluate_model(model, processor, test_samples)\n",
    "    \n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4de48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 12. Chạy\n",
    "# ========================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"QWEN2-VL VIETNAMESE HERITAGE VQA TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "\n",
    "    print(\"\\nBắt đầu training...\")\n",
    "    test_metrics = train()\n",
    "    print(\"\\nHOÀN THÀNH!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ca219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# Load từ CHECKPOINT-423 thay vì folder gốc\n",
    "CHECKPOINT_PATH = \"/kaggle/working/qwen2vl-heritage-vqa/checkpoint-423\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "processor.tokenizer.padding_side = 'left'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# Load từ checkpoint-423\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Reset generation config\n",
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_p = None  \n",
    "model.generation_config.top_k = None\n",
    "model.generation_config.repetition_penalty = 1.2\n",
    "\n",
    "print(\"✓ Model loaded from checkpoint-423!\")\n",
    "\n",
    "# Test\n",
    "img = Image.open(\"/kaggle/input/images/images/can_tho/can_cu_vuon_man/19.png\").convert('RGB')\n",
    "img.thumbnail((336, 336), Image.Resampling.LANCZOS)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "    {\"type\": \"image\", \"image\": img},\n",
    "    {\"type\": \"text\", \"text\": \"Điều gì đặc biệt về những kỷ vật được trưng bày trong bức ảnh?\"}\n",
    "]}]\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, _ = process_vision_info(messages)\n",
    "inputs = processor(text=[text], images=image_inputs, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, do_sample=False, \n",
    "                             pad_token_id=processor.tokenizer.pad_token_id)\n",
    "\n",
    "answer = processor.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "print(f\"Trả lời: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
